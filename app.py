# app.py
"""
Streamlit-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ: –±—ã—Å—Ç—Ä—ã–π –∏ —Ç–æ—á–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏ / –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –æ–±—Ä–∞—â–µ–Ω–∏–π.
–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ TF-IDF, blocking, NearestNeighbors, union-find –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞.
–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤—Ö–æ–¥–Ω—ã–µ —Ñ–∞–π–ª—ã –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ ./process_data/
"""

import streamlit as st
import pandas as pd
import numpy as np
import re
from pathlib import Path
from rapidfuzz import fuzz
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.neighbors import NearestNeighbors
import nltk
from nltk.corpus import stopwords
import plotly.express as px
import datetime
import io
from typing import Tuple, List, Iterable, Dict
import openpyxl

# Ensure stopwords
try:
    stopwords.words('russian')
except LookupError:
    nltk.download('stopwords')

# ---------- Config ----------
PROCESS_DIR = Path("./process_data")
PROCESS_DIR.mkdir(parents=True, exist_ok=True)

PHONE_PATTERN = re.compile(r'(?:\+7|8)?\s*\(?\d{3}\)?[\s\-]?\d{3}[\s\-]?\d{2}[\s\-]?\d{2}')
# Precompiled simple non-digit removal
NON_DIGIT = re.compile(r'\D')

# ---------- Utilities ----------
def extract_phone_numbers_frozen(text) -> frozenset:
    """–ù–∞–π—Ç–∏ —Ç–µ–ª–µ—Ñ–æ–Ω—ã –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å –∫–∞–∫ +7XXXXXXXXXX. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç frozenset –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è."""
    if pd.isna(text):
        return frozenset()
    s = str(text)
    matches = PHONE_PATTERN.findall(s)
    normalized = []
    for m in matches:
        d = NON_DIGIT.sub('', m)
        if len(d) == 11 and d.startswith('8'):
            d = '7' + d[1:]
        if len(d) == 11 and d.startswith('7'):
            normalized.append('+' + d)
        elif len(d) == 10:
            normalized.append('+7' + d)
    return frozenset(normalized)

def detect_key_columns(df: pd.DataFrame) -> Dict[str, str]:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–æ–Ω–∫–∏ name/text/address –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—è–º; –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –∏–º–µ–Ω –∫–æ–ª–æ–Ω–æ–∫ (None –µ—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ)."""
    name_col = text_col = addr_col = None
    for c in df.columns:
        lc = str(c).lower()
        if not name_col and any(k in lc for k in ['–ø–æ—Ç—Ä–µ–±', '–∫–ª–∏–µ–Ω—Ç', '–∑–∞—è–≤–∏—Ç–µ–ª', '–∏–º—è', '—Ñ–∞–º–∏–ª–∏—è', '–∫–æ–Ω—Ç–∞–∫—Ç']):
            name_col = c
        if not text_col and any(k in lc for k in ['—Ç–µ–∫—Å—Ç', '–æ–±—Ä–∞—â', '–æ–ø–∏—Å–∞–Ω', '–∂–∞–ª–æ–±', '—Å–æ–æ–±—â–µ–Ω']):
            text_col = c
        if not addr_col and any(k in lc for k in ['–∞–¥—Ä–µ—Å', '–º–µ—Å—Ç–æ–ø–æ–ª–æ–∂', '–æ–±—ä–µ–∫—Ç', '—É–ª–∏—Ü–∞', '–¥–æ–º']):
            addr_col = c
    # fallback: –ø–µ—Ä–≤—ã–π object —Å—Ç–æ–ª–±–µ—Ü –¥–ª—è —Ç–µ–∫—Å—Ç–∞
    if text_col is None:
        for c in df.columns:
            if df[c].dtype == object:
                text_col = c
                break
    return {'name': name_col, 'text': text_col, 'address': addr_col}

# ---------- TF-IDF building & neighbors ----------
@st.cache_data(show_spinner=False)
def build_tfidf(texts: Iterable[str]) -> Tuple[TfidfVectorizer, 'scipy.sparse.csr_matrix']:
    """–ü–æ—Å—Ç—Ä–æ–∏—Ç—å TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä –∏ –º–∞—Ç—Ä–∏—Ü—É (–∫—ç—à–∏—Ä—É–µ—Ç—Å—è Streamlit)."""
    vec = TfidfVectorizer(stop_words=stopwords.words('russian'), max_df=0.95, min_df=1)
    tfidf = vec.fit_transform((str(t).lower() for t in texts))
    return vec, tfidf

def find_tfidf_candidates(tfidf_matrix, top_k=5, similarity_threshold=0.4) -> List[tuple]:
    """
    –î–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–∏ –Ω–∞—Ö–æ–¥–∏–º top_k —Å–æ—Å–µ–¥–µ–π (–ø–æ cosine distance —á–µ—Ä–µ–∑ NearestNeighbors) –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º
    —Å–ø–∏—Å–æ–∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ (i, j, sim) —Å sim = cosine similarity.
    """
    n = tfidf_matrix.shape[0]
    if n <= 1:
        return []
    k = min(top_k + 1, n)  # +1 –ø–æ—Ç–æ–º—É —á—Ç–æ —Å–∞–º —Å–æ—Å–µ–¥
    nn = NearestNeighbors(n_neighbors=k, metric='cosine', algorithm='brute')
    nn.fit(tfidf_matrix)
    distances, indices = nn.kneighbors(tfidf_matrix)
    candidates = []
    for i in range(n):
        for dist, j in zip(distances[i], indices[i]):
            if i >= j:
                continue
            sim = 1.0 - dist  # NearestNeighbors cosine distance = 1 - cos_sim
            if sim >= similarity_threshold:
                candidates.append((i, j, float(sim)))
    return candidates

# ---------- Union-Find (DSU) for grouping ----------
class DSU:
    def __init__(self, n):
        self.parent = list(range(n))
        self.rank = [0]*n

    def find(self, x):
        while self.parent[x] != x:
            self.parent[x] = self.parent[self.parent[x]]
            x = self.parent[x]
        return x

    def union(self, a, b):
        ra, rb = self.find(a), self.find(b)
        if ra == rb:
            return
        if self.rank[ra] < self.rank[rb]:
            self.parent[ra] = rb
        else:
            self.parent[rb] = ra
            if self.rank[ra] == self.rank[rb]:
                self.rank[ra] += 1

    def components(self):
        comp = {}
        for i in range(len(self.parent)):
            r = self.find(i)
            comp.setdefault(r, []).append(i)
        return comp

# ---------- Core detection (optimized) ----------
def detect_duplicates_optimized(
    df: pd.DataFrame,
    tfidf_threshold=0.45,
    top_k=5,
    name_fuzzy_threshold=85,
    addr_fuzzy_threshold=75,
    score_threshold=12,
    weights=None,
    block_on_phone=True,
    block_on_name_prefix=True
):
    """
    –û—Å–Ω–æ–≤–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è:
    - –ü—Ä–µ–¥–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç: –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–ª–µ—Ñ–æ–Ω—ã (frozenset), –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Ç–µ–∫—Å—Ç/–∏–º—è/addr.
    - –°—Ç—Ä–æ–∏—Ç TF-IDF –æ–¥–∏–Ω —Ä–∞–∑.
    - –ü–æ–ª—É—á–∞–µ—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ —á–µ—Ä–µ–∑ TF-IDF + —Ç–µ–ª–µ—Ñ–æ–Ω–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è.
    - –î–ª—è –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –≤—ã—á–∏—Å–ª—è–µ—Ç fuzzy-–æ—Ü–µ–Ω–∫–∏ (rapidfuzz) –∏ —Å—É–º–º–∞—Ä–Ω—ã–π –±–∞–ª–ª.
    - –ì—Ä—É–ø–ø–∏—Ä—É–µ—Ç –ø–∞—Ä—ã —á–µ—Ä–µ–∑ DSU.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: unique_df, duplicates_df, work_df (work_df —Å–æ–¥–µ—Ä–∂–∏—Ç —Å–ª—É–∂–µ–±–Ω—ã–µ –ø–æ–ª—è).
    """
    if weights is None:
        weights = {'phones': 15, 'tfidf': 8, 'name': 5, 'address': 4}

    df_work = df.copy().reset_index(drop=True)
    n = len(df_work)
    cols = detect_key_columns(df_work)
    name_col = cols['name']
    text_col = cols['text']
    addr_col = cols['address']

    df_work['_name'] = df_work[name_col].fillna('').astype(str) if name_col else pd.Series(['']*n)
    df_work['_text'] = df_work[text_col].fillna('').astype(str) if text_col else pd.Series(['']*n)
    df_work['_addr'] = df_work[addr_col].fillna('').astype(str) if addr_col else pd.Series(['']*n)

    # phones: –∏–∑–≤–ª–µ—á—å –ø–æ –≤—Å–µ–π —Å—Ç—Ä–æ–∫–µ –∏ —Å–¥–µ–ª–∞—Ç—å frozenset
    df_work['_phones'] = df_work.apply(lambda r: extract_phone_numbers_frozen(' '.join(map(str, r.values))), axis=1)

    # text list for TF-IDF
    texts = df_work['_text'].fillna('').astype(str).tolist()
    vec, tfidf = build_tfidf(texts)

    # Candidates set
    candidate_pairs = set()

    # 1) TF-IDF-based neighbors
    tfidf_cands = find_tfidf_candidates(tfidf, top_k=top_k, similarity_threshold=tfidf_threshold)
    for i, j, sim in tfidf_cands:
        candidate_pairs.add((i, j))

    # 2) Phone-based exact matches (blocking): –µ—Å–ª–∏ —Ç–µ–ª–µ—Ñ–æ–Ω—ã —Å–æ–≤–ø–∞–¥–∞—é—Ç ‚Äî –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π –∫–∞–Ω–¥–∏–¥–∞—Ç
    if block_on_phone:
        phone_map = {}
        for idx, phones in enumerate(df_work['_phones']):
            for ph in phones:
                phone_map.setdefault(ph, []).append(idx)
        for ph, idxs in phone_map.items():
            if len(idxs) > 1:
                # add all pairs within idxs
                for a_idx in range(len(idxs)):
                    for b_idx in range(a_idx + 1, len(idxs)):
                        candidate_pairs.add((idxs[a_idx], idxs[b_idx]))

    # 3) Blocking by name prefix (optional) ‚Äî —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º –≤–Ω—É—Ç—Ä–∏ –º–∞–ª–µ–Ω—å–∫–∏—Ö –±–ª–æ–∫–æ–≤
    if block_on_name_prefix:
        name_buckets = {}
        for idx, nm in enumerate(df_work['_name']):
            key = (str(nm).strip()[:2].lower() if nm else '')
            name_buckets.setdefault(key, []).append(idx)
        for key, idxs in name_buckets.items():
            if 1 < len(idxs) <= 50:  # –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –±–ª–æ–∫–∞
                for a in range(len(idxs)):
                    for b in range(a+1, len(idxs)):
                        candidate_pairs.add((idxs[a], idxs[b]))

    # Evaluate candidate pairs with fuzzy and tfidf similarity (fetch tfidf similarity on demand)
    dsu = DSU(n)
    pair_scores = {}  # (i,j) -> score and criteria
    total_pairs = len(candidate_pairs)
    # progress bar
    prog = st.session_state.get("_prog_obj")
    if prog is None:
        prog = None

    i_pair = 0
    for (i, j) in candidate_pairs:
        i_pair += 1
        # update progress if provided
        if prog:
            prog.progress(min(int(i_pair / max(1, total_pairs) * 100), 100))
        score = 0
        criteria = []

        # phones
        phones_i = df_work.at[i, '_phones']
        phones_j = df_work.at[j, '_phones']
        if phones_i and phones_j and (set(phones_i) & set(phones_j)):
            score += weights['phones']; criteria.append('phone')

        # tfidf similarity (compute quickly via vectors)
        # compute cosine similarity using dot product of normalized TF-IDF rows
        try:
            vec_i = tfidf[i]
            vec_j = tfidf[j]
            # cosine similarity via dot (since sklearn uses normalized tf-idf by default)
            sim = float((vec_i @ vec_j.T).A1[0])  # .A1 to get scalar
        except Exception:
            sim = 0.0
        if sim >= tfidf_threshold:
            score += weights['tfidf']; criteria.append(f'tfidf:{sim:.2f}')

        # fuzzy name
        n1 = df_work.at[i, '_name']
        n2 = df_work.at[j, '_name']
        if n1 and n2:
            name_score = fuzz.token_set_ratio(str(n1), str(n2))
            if name_score >= name_fuzzy_threshold:
                score += weights['name']; criteria.append(f'name:{int(name_score)}')

        # fuzzy address
        a1 = df_work.at[i, '_addr']
        a2 = df_work.at[j, '_addr']
        if a1 and a2:
            addr_score = fuzz.token_sort_ratio(str(a1), str(a2))
            if addr_score >= addr_fuzzy_threshold:
                score += weights['address']; criteria.append(f'addr:{int(addr_score)}')

        if score >= score_threshold:
            # mark as duplicate: union them
            dsu.union(i, j)
            pair_scores[(i, j)] = {'score': score, 'criteria': criteria, 'tfidf': sim}

    # build groups from DSU components
    comps = dsu.components()
    group_map = {}
    group_id = 0
    for root, members in comps.items():
        # if single member and never joined with anyone with score -> it might be unique
        if len(members) == 1:
            # only assign group to singletons if they were in pair_scores (i.e., flagged)
            # but DSU might have left them as single if not unioned ‚Äî they remain unique
            continue
        group_id += 1
        for m in members:
            group_map[m] = f"G{group_id}"

    # Create work_df with group labels and summary
    df_work['–ì—Ä—É–ø–ø–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤'] = df_work.index.map(lambda x: group_map.get(x, ''))
    # Add criteria and score aggregated per row (sum of pairwise scores with others in group)
    df_work['–°—É–º–º–∞—Ä–Ω—ã–π –±–∞–ª–ª —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è'] = 0.0
    df_work['–ö—Ä–∏—Ç–µ—Ä–∏–π —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è'] = ''

    # For each pair that was accepted, annotate rows
    for (i, j), info in pair_scores.items():
        gi = group_map.get(i, '')
        gj = group_map.get(j, '')
        if gi:
            # accumulate on each member
            df_work.at[i, '–°—É–º–º–∞—Ä–Ω—ã–π –±–∞–ª–ª —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è'] += info['score']
            df_work.at[i, '–ö—Ä–∏—Ç–µ—Ä–∏–π —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è'] = ', '.join(set(filter(None, [df_work.at[i, '–ö—Ä–∏—Ç–µ—Ä–∏–π —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è'], *info['criteria']])))
        if gj:
            df_work.at[j, '–°—É–º–º–∞—Ä–Ω—ã–π –±–∞–ª–ª —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è'] += info['score']
            df_work.at[j, '–ö—Ä–∏—Ç–µ—Ä–∏–π —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è'] = ', '.join(set(filter(None, [df_work.at[j, '–ö—Ä–∏—Ç–µ—Ä–∏–π —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è'], *info['criteria']])))

    # duplicates_df: rows that have a group id
    duplicates_df = df_work[df_work['–ì—Ä—É–ø–ø–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤'] != ''].copy().reset_index(drop=True)
    unique_df = df_work[df_work['–ì—Ä—É–ø–ø–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤'] == ''].copy().reset_index(drop=True)

    # stop progress
    if prog:
        prog.progress(100)

    return unique_df, duplicates_df, df_work

# ---------- Report generation ----------
def generate_report_excel(output_path: Path, original_df: pd.DataFrame, unique_df: pd.DataFrame, duplicates_df: pd.DataFrame):
    try:
        stats = [
            {'–ü–æ–∫–∞–∑–∞—Ç–µ–ª—å': '–í—Å–µ–≥–æ –æ–±—Ä–∞—â–µ–Ω–∏–π', '–ó–Ω–∞—á–µ–Ω–∏–µ': int(len(original_df))},
            {'–ü–æ–∫–∞–∑–∞—Ç–µ–ª—å': '–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –æ–±—Ä–∞—â–µ–Ω–∏–π', '–ó–Ω–∞—á–µ–Ω–∏–µ': int(len(unique_df))},
            {'–ü–æ–∫–∞–∑–∞—Ç–µ–ª—å': '–î—É–±–ª–∏–∫–∞—Ç–æ–≤ (–∑–∞–ø–∏—Å–µ–π)', '–ó–Ω–∞—á–µ–Ω–∏–µ': int(len(duplicates_df))},
            {'–ü–æ–∫–∞–∑–∞—Ç–µ–ª—å': '–î–æ–ª—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ (%)',
             '–ó–Ω–∞—á–µ–Ω–∏–µ': f"{(len(duplicates_df)/len(original_df)*100):.1f}%" if len(original_df) else "0%"}
        ]
        stats_df = pd.DataFrame(stats)
        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
            original_df.to_excel(writer, sheet_name='–ò—Å—Ö–æ–¥–Ω—ã–µ', index=False)
            unique_df.to_excel(writer, sheet_name='–£–Ω–∏–∫–∞–ª—å–Ω—ã–µ', index=False)
            duplicates_df.to_excel(writer, sheet_name='–î—É–±–ª–∏–∫–∞—Ç—ã', index=False)
            stats_df.to_excel(writer, sheet_name='–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞', index=False)
        return True, None
    except Exception as e:
        return False, str(e)

# ---------- Streamlit UI ----------
st.set_page_config(page_title="–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –æ–±—Ä–∞—â–µ–Ω–∏–π", layout="wide")
st.title("üìä –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –æ–±—Ä–∞—â–µ–Ω–∏–π ‚Äî —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å –∏ –¥—É–±–ª–∏–∫–∞—Ç—ã")
st.markdown(
    """
    –ó–∞–≥—Ä—É–∑–∏—Ç–µ Excel-—Ñ–∞–π–ª (.xlsx / .xls). –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –≤ `process_data/`.
    –ê–ª–≥–æ—Ä–∏—Ç–º –∏—Å–ø–æ–ª—å–∑—É–µ—Ç TF-IDF + rapidfuzz + phone-blocking + union-find –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –≥—Ä—É–ø–ø –¥—É–±–ª–∏–∫–∞—Ç–æ–≤.
    """
)

uploaded = st.file_uploader("–í—ã–±–µ—Ä–∏—Ç–µ Excel-—Ñ–∞–π–ª", type=["xlsx", "xls"])
if not uploaded:
    st.info("–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª —á—Ç–æ–±—ã –Ω–∞—á–∞—Ç—å.")
    st.stop()

# Save uploaded file
ts = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
input_path = PROCESS_DIR / f"{ts}_{uploaded.name}"
with open(input_path, "wb") as f:
    f.write(uploaded.getbuffer())
st.success(f"–§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω—ë–Ω: `{input_path}`")

# Try load file
try:
    df = pd.read_excel(input_path)
except Exception as e:
    st.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è Excel: {e}")
    st.stop()

st.subheader("–ü—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä –¥–∞–Ω–Ω—ã—Ö")
st.dataframe(df.head(10))

# Panel: thresholds
with st.expander("–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è (—Ç–æ–Ω–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞)"):
    col1, col2 = st.columns(2)
    with col1:
        tfidf_thr = st.slider("TF-IDF threshold (similarity)", 0.0, 1.0, 0.45, 0.01)
        top_k = st.slider("TF-IDF neighbors (top_k)", 1, 20, 5)
        score_thr = st.slider("–ü–æ—Ä–æ–≥ —Å—É–º–º–∞—Ä–Ω–æ–≥–æ –±–∞–ª–ª–∞", 1, 100, 12)
    with col2:
        name_thr = st.slider("Fuzzy –∏–º—è (threshold)", 50, 100, 85)
        addr_thr = st.slider("Fuzzy –∞–¥—Ä–µ—Å (threshold)", 50, 100, 75)
        phones_block = st.checkbox("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–ª–æ–∫–∏—Ä–æ–≤–∫—É –ø–æ —Ç–µ–ª–µ—Ñ–æ–Ω–∞–º (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)", value=True)
        name_block = st.checkbox("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–ª–æ–∫–∏—Ä–æ–≤–∫—É –ø–æ –ø—Ä–µ—Ñ–∏–∫—Å—É –∏–º–µ–Ω–∏", value=True)

# Start processing
if st.button("‚ñ∂Ô∏è –ó–∞–ø—É—Å—Ç–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É"):
    # create a progress object in session to be used inside function
    prog = st.progress(0)
    st.session_state["_prog_obj"] = prog

    try:
        unique_df, duplicates_df, work_df = detect_duplicates_optimized(
            df,
            tfidf_threshold=tfidf_thr,
            top_k=top_k,
            name_fuzzy_threshold=name_thr,
            addr_fuzzy_threshold=addr_thr,
            score_threshold=score_thr,
            weights={'phones': 15, 'tfidf': 8, 'name': 5, 'address': 4},
            block_on_phone=phones_block,
            block_on_name_prefix=name_block
        )

        # cleanup session progress
        prog.empty()
        if "_prog_obj" in st.session_state:
            del st.session_state["_prog_obj"]

        # Metrics
        total = len(df)
        uniq = len(unique_df)
        dups = len(duplicates_df)
        st.subheader("–ö–ª—é—á–µ–≤—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏")
        c1, c2, c3, c4 = st.columns(4)
        c1.metric("–í—Å–µ–≥–æ –æ–±—Ä–∞—â–µ–Ω–∏–π", f"{total}")
        c2.metric("–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö", f"{uniq}")
        c3.metric("–î—É–±–ª–∏–∫–∞—Ç–æ–≤", f"{dups}")
        c4.metric("–î–æ–ª—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤", f"{(dups/total*100):.1f}%" if total else "0%")

        # Show examples
        st.subheader("–ü—Ä–∏–º–µ—Ä—ã —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π")
        st.dataframe(unique_df.head(10))

        st.subheader("–ü—Ä–∏–º–µ—Ä—ã –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ (—Å –≥—Ä—É–ø–ø–∞–º–∏)")
        # show sample with group id and criteria
        display_cols = list(df.columns) + ['–ì—Ä—É–ø–ø–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤', '–ö—Ä–∏—Ç–µ—Ä–∏–π —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è', '–°—É–º–º–∞—Ä–Ω—ã–π –±–∞–ª–ª —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è']
        st.dataframe(duplicates_df[display_cols].head(50))

        # Charts
        st.subheader("–ì—Ä–∞—Ñ–∏–∫–∏ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏")
        # detect date column
        date_col = next((c for c in df.columns if "–¥–∞—Ç–∞" in str(c).lower() or "date" in str(c).lower()), None)
        if date_col:
            try:
                df[date_col] = pd.to_datetime(df[date_col], errors='coerce')
                monthly = df[df[date_col].notna()][date_col].dt.to_period('M').value_counts().sort_index()
                if not monthly.empty:
                    fig_month = px.bar(x=monthly.index.astype(str), y=monthly.values,
                                       labels={"x": "–ú–µ—Å—è—Ü", "y": "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ"},
                                       title="–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–±—Ä–∞—â–µ–Ω–∏–π –ø–æ –º–µ—Å—è—Ü–∞–º")
                    st.plotly_chart(fig_month, use_container_width=True)

                valid = df[df[date_col].notna()].copy()
                valid['dow_en'] = valid[date_col].dt.day_name()
                dow_map = {
                    'Monday': '–ü–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫', 'Tuesday': '–í—Ç–æ—Ä–Ω–∏–∫', 'Wednesday': '–°—Ä–µ–¥–∞',
                    'Thursday': '–ß–µ—Ç–≤–µ—Ä–≥', 'Friday': '–ü—è—Ç–Ω–∏—Ü–∞', 'Saturday': '–°—É–±–±–æ—Ç–∞', 'Sunday': '–í–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ'
                }
                valid['–î–µ–Ω—å –Ω–µ–¥–µ–ª–∏'] = valid['dow_en'].map(dow_map)
                day_stats = valid['–î–µ–Ω—å –Ω–µ–¥–µ–ª–∏'].value_counts().reindex(
                    ['–ü–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫','–í—Ç–æ—Ä–Ω–∏–∫','–°—Ä–µ–¥–∞','–ß–µ—Ç–≤–µ—Ä–≥','–ü—è—Ç–Ω–∏—Ü–∞','–°—É–±–±–æ—Ç–∞','–í–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ']
                ).fillna(0)
                if day_stats.sum() > 0:
                    fig_day = px.bar(x=day_stats.index, y=day_stats.values,
                                     labels={"x": "–î–µ–Ω—å –Ω–µ–¥–µ–ª–∏", "y": "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ"},
                                     title="–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–±—Ä–∞—â–µ–Ω–∏–π –ø–æ –¥–Ω—è–º –Ω–µ–¥–µ–ª–∏")
                    st.plotly_chart(fig_day, use_container_width=True)
            except Exception as e:
                st.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—É—é –∞–Ω–∞–ª–∏—Ç–∏–∫—É: {e}")
        else:
            st.info("–ù–µ –Ω–∞–π–¥–µ–Ω —Å—Ç–æ–ª–±–µ—Ü —Å –¥–∞—Ç–æ–π –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏.")

        # Category pie if any
        cat_col = next((c for c in df.columns if '–∫–∞—Ç–µ–≥' in str(c).lower() or '—Ç–∏–ø' in str(c).lower()), None)
        if cat_col:
            cat_stats = df[cat_col].fillna('–ù–µ —É–∫–∞–∑–∞–Ω–æ').value_counts()
            if not cat_stats.empty:
                fig_cat = px.pie(values=cat_stats.values, names=cat_stats.index, title="–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º")
                st.plotly_chart(fig_cat, use_container_width=True)

        # Save report and provide download
        out_name = f"–æ—Ç—á–µ—Ç_–æ–±—Ä–∞—â–µ–Ω–∏–π_{ts}.xlsx"
        out_path = PROCESS_DIR / out_name
        ok, err = generate_report_excel(out_path, df, unique_df, duplicates_df)
        if ok:
            st.success(f"–û—Ç—á—ë—Ç —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω: `{out_path}`")
            with open(out_path, "rb") as f:
                st.download_button("‚¨áÔ∏è –°–∫–∞—á–∞—Ç—å –æ—Ç—á—ë—Ç (Excel)", f, file_name=out_path.name,
                                   mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
        else:
            st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –æ—Ç—á—ë—Ç–∞: {err}")

    except Exception as e:
        # cleanup progress
        try:
            prog.empty()
        except Exception:
            pass
        if "_prog_obj" in st.session_state:
            del st.session_state["_prog_obj"]
        st.error(f"–û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")

